---
layout: page
title: Minimal Understanding of MCMC
nav: false
---
<link rel="stylesheet" href="/assets/css/main.css"/>

## Introduction

Many times we can estimate a statistic from a given distribution \\(X\\),
drawing random i.i.d samples from it. For example, the mean

\\[
\frac{1}{n}\sum_i f(X_i) \sim \textbf{E}(f(X))
\tag{1.1}
\\]

The law of the large numbers, ensure we'll get a decent estimation if enough
samples are drawn. This method works well until we can no longer sample from
the distribution. This can sound a bit harsh, but it's a pretty common
situation in bayesian inference when we try to compute the posterior
distribution. Even in simple situations the posterior can't be calculated.
Consider the model

```python
import pyro
import pyro.distributions as dist

def model(): 
    mu = pyro.sample("mu", dist.HalfCauchy(scale=1)) 
    return pyro.sample("obs", dist.Normal(mu, 1)) 
```
<br>

Imagine we condition this model on some data. We can always compute the bayes
numerator, our problem is the evidence


\\[
\textbf{P}(X) = \int_0^{\infty}
\frac{1}{1+\mu^2}\exp\left(-\frac{1}{2}\sum_{i=1}^{n} (\mu - x_i)^2\right)d\mu
\tag{1.2}
\\]

Which is an intractable integral. This is just to estimate one parameter, so we
can imagine what could happen when there is a bunch of them.

Given this problem, we're going to review the Markov Chain Monte Carlo (MCMC)
algorithm, that allow us to approximate those distributions and sample from
them. Even though MCMC is pretty old, still remains as the way to go to resolve
this problem. Popular packages, such as facebook prophet, were developed using
bayesian inference with MCMC.

## What is MCMC?

There are several versions of MCMC, but in general the idea is to generate a
Markov Chain whose stationary distribution is the one we want to sample
from. 

The chain is generated by proposing candidates from another distribution
\\(q\\) (the proposal) and then accept it or reject it according to a
probability \\(\alpha\\).

1. If we are at \\(X_n = x\\), sample \\(y\\) from \\(q(x, y)\\)
2. Compute \\(\alpha(x,y)\\) 
3. Accept the proposal with probability \\(\alpha(x,y)\\). If accepted, set
\\(X_{n+1} = y\\) else \\(X_{n+1}=x\\)

One of the earlier MCMC methods it's called Metropolis-Hasting. For this 
particular algorithm, the \\(\alpha\\) is given by

\\[
\alpha(x,y) = \min\left(1,\frac{q(y,x)\pi(y)}{q(x,y)\pi(x)}\right)
\tag{1.3}
\\]

One of the main advantages is we don't need to know a normalized version of the
density \\(\pi\\) because every constant gets cancel out in the quotient.

We'll review more details about why and how MCMC works, but first let's take a
look at the next example

## Toy example

Suppose we want to apply MCMC to sample from a distribution with finite
support. This isn't the most likely case someone would use the algorithm but
it's much easier to illustrate it on this way.

Let's compute the entries of the transition matrix.  If \\(i \neq j\\) the
probability of jumping from \\(i\\) to \\(j\\) can be interpreted as picking
\\(j\\) and the accept it. As those process are independent we have

\\[
p_{ij}=\textbf{P}[X_{n+1}=i|X_n=j]=q(i,j)\alpha(i,j)
\tag{1.4}
\\]

As the rows sum in total one, the simplest way to compute \\(p_{ii}\\) is through
the complement 

\\[
p_{ii} = 1 - \sum_{i\neq j} p_{ij}
\tag{1.5}
\\]

Next we're going to construct the whole matrix for the particular case of MH.

```python
def compute_transition_matrix(pi, q, size):
    alpha = lambda x, y: \
        min(1, (pi(y) * q(y,x)) / (pi(x) * q(x,y))) if x != y else 0
    fn = lambda x, y: q(x, y) * alpha(x, y)  
    g = np.indices((size, size))
    P = np.vectorize(fn, otypes=[np.float64])(g[0], g[1])
    np.fill_diagonal(P, 1 - P.sum(axis=1))
    return P
```
<br>

Suppose now we're trying to sample from \\(bin(n, p)\\) (state space
\\(S=\{0,\ldots, n\}\\))

```python
class binom_mcmc:
    def __init__(self, n, p):
        self.n = n
	self.dist = binom(n, p)
		  
    def _pi(self, *args):
	return self.dist.pmf(*args)
				       
    @property
    def pi(self):
	return self._pi(range(self.n + 1))
	     
    def transition_matrix(self, q=None):
	if not q: q = self.unif_q
        return compute_transition_matrix(self._pi, q, self.n + 1)
		
    def unif_q(self, *args):
	return 1 / (self.n + 1)
```
<br>

Take a look at the matrix when \\(n=3\\) and \\(p=.4\\).

```python
bm = binom_mcmc(3, .4)

P = bm.transition_matrix()
```
<br>
This \\(P\\) looks like 

$$
\begin{bmatrix}
0.4259 & 0.25 & 0.25 & 0.0740\\
0.125 & 0.6712 & 0.1667 & 0.0370\\
0.1875 & 0.25 & 0.5069 & 0.0556\\
0.25 & 0.25 & 0.25 & 0.25
\end{bmatrix}
$$

If we take a closer look at the matrix \\(P\\) we can distinguish a pattern.
Look for example at the second row which represents the probability of jumping
from state 1 to another one.  As \\(p_{11}\\) is the higher probability on that
row, staying at 1 is the most likely outcome. In contrast with jumping to the
state 4 because \\(p_{14}\\) is the lowest probability there. That means that
the chain will stay more time on the state 1 and less in the state 4. If we
plot an histogram with the states visited, we will see more mass on 1, and in
general, the shape of the histogram will coincide with the distribution
\\(\pi=\(0.216, 0.432, 0.288, 0.064)\\)

Let's verify if \\(\pi\\) is a stationary distribution for \\(P\\)

```python
assert np.allclose(bm.pi @ P, bm.pi)
print('Test passed')
```
`Test passed`

We can check with a different proposal too

```python
def binom_q(value, *args):
    return binom(3, .7).pmf(value)

Q = bm.transition_matrix(binom_q)
assert np.allclose(bm.pi @ P, bm.pi)
print('Test passed')
```
`Test passed`

In the next section we'll try to understand more formally why the Markov Chain
has as limit distribution the one we want.

## Why the chain converge?

The Markov chain is constructed to fulfil a different condition, one sufficient
but not necessary for the convergence. It's called *detailed balanced*

\\[
\pi(x)p(x,y)=\pi(y)p(y,x)
\tag{1.6}
\\]

It can be interpreted as the probability of standing over a state \\(x\\) and
then jumping to \\(y\\) it's equal to start at \\(y\\) and then jump to
\\(x\\). It isn't hard to prove that detailed balanced implies the distribution
is stationary. (Here we're missing a lot of details, for example the chain also
needs to be aperiodic and irreducible. They can be consulted elsewhere and
aren't that relevant to get a decent intuition on how the algorithm works)

On the particular case of MCMC, we need
\\[
\pi(x)q(x,y)\alpha(x,y) = \pi(y)q(y,x)\alpha(y,x)
\tag{1.7}
\\]

Which can be seen as 

\\[
\frac{\alpha(x,y)}{\alpha(y,x)} = \frac{\pi(y)q(y,x)}{\pi(x)q(x,y)}
\tag{1.8}
\\]

Any acceptance function should be chosen according to this equation. In the
particular case of Metropolis-Hasting, we simple set \\(\alpha(x,y)=1\\) or
\\(\alpha(y,x)=1\\) depending on the quotient on the right. If

\\[
\pi(x)q(x,y) > \pi(y)q(y,x)
\\]

Then \\(\alpha(y,x)=1\\) and \\(\alpha(x,y) = \pi(y)q(y,x)/\pi(x)q(x,y)\\),
else, the inverse. Of course, this can be rewritten as `(1.3)` . Even though this
function is nothing fancy, still offers a good trade-off between exploring new
states and staying at high density areas.
